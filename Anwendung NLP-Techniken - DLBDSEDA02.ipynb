{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84372080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anwendung von NLP-Techniken\n",
    "\n",
    "# Import der erforderlichen Bibliotheken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Laden der Daten (csv.-Datei). Möglichst die Datei in selben Ordner wie den Programmcode legen\n",
    "# Ansonsten Pfad evtl. entsprechend anpassen\n",
    "df = pd.read_csv(\"rows.csv\", low_memory=False)\n",
    "df.shape\n",
    "\n",
    "# Es erscheint eine Warnung welche hier ignoriert werden kann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef545995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der Datensatz besteht aus über eine Millionen Zeilen und 18 Spalten\n",
    "# Anzeige der ersten 3 Zeilen der Daten und Transponieren des Ergebnisses\n",
    "df.head(3).T\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c084b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entfernung irrelevanter Daten wie Adressen oder Datum, um den Datensatz zu verkleinern\n",
    "# Der kleinere Datensatz wird neu abgespeichert\n",
    "df1 = df[['Product', 'Consumer complaint narrative']].copy()\n",
    "\n",
    "# Entfernen der Datensätze, welche keine Beschwerde enthalten, also unter \"Consumer complaint narrative\" ein \"NaN\"\n",
    "df1 = df1[pd.notnull(df1['Consumer complaint narrative'])]\n",
    "\n",
    "# Einfachere Benennung der zwei Spalten einfügen\n",
    "df1.columns = ['Product', 'Complaint'] \n",
    "\n",
    "# Ausgabe er Anzahl übriger Beschwerden zur weiteren Verarbeitung\n",
    "df1.shape\n",
    "\n",
    "# Optional Visualisierung der Daten\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8e3cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der verkleinerte Datensatz besteht aus über 383000 Beschwerden und 2 Spalten \n",
    "# Optional zur Visualisierung der Daten\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f71070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeige der Unterschiedlichen Kategorien (Product) aus dem Datensatz (optional)\n",
    "# pd.DataFrame(df.Product.unique()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeige der Anzahl der Kategorien (Product) aus dem Datensatz\n",
    "count_unique_products = df.Product.nunique()\n",
    "print(count_unique_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f161b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier die Anzeige der verschiedenen Kategorien und Anzahl der Beschwerden dazu in absteigender Reihenfolge\n",
    "df1['Product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704dc2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Da der Datensatz noch sehr groß ist und mein System an seine Grenzen in Bezug auf die Rechenleistung kommt \n",
    "# wird nun lediglich die Kategorie \"Student loan\" weiter verarbeitet\n",
    "# Beispielhafte Darstellung der ersten 5 Einträge\n",
    "df2 = df1[df1['Product'] == 'Student loan']\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6181c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der erforderlichen Bibliothek für das Stemming\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Erstellen des Stemmers\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Definition einer Funktion, um das Stemming anzuwenden\n",
    "def stem_sentence(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Funktion auf die Spalte 'Complaint' anwenden\n",
    "df2 = df2.copy()\n",
    "df2['Complaint'] = df2['Complaint'].astype(str)\n",
    "df2['Complaint'] = df2['Complaint'].apply(stem_sentence)\n",
    "\n",
    "# Visualisierung des Ergebnisses\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516fd22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Als Alternative kann eine Stichprobe zur weiteren Verkleinerung des Datensazues gezogen werden\n",
    "# Falls genügend Rechenkapazität zur verfügung steht kann dieser Schritt übersprungen werden\n",
    "# und der Code muss entsprechend auf den Ursprungsdatensatz angepasst werden\n",
    "# df2 = df2.copy(10000, random_state=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848a3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative wenn alle Kategorien genutzt werden sollen,\n",
    "# Da es ähnliche Kategorien gibt, werden diese zusammengefasst und entsprechend umbenannt\n",
    "# df2.replace({'Product': \n",
    "  #            {'Credit reporting, credit repair services, or other personal consumer reports': \n",
    "  #            'Credit reporting, repair, or other', \n",
    "  #            'Credit reporting': 'Credit reporting, repair, or other',\n",
    "  #           'Credit card': 'Credit card or prepaid card',\n",
    "  #           'Prepaid card': 'Credit card or prepaid card',\n",
    "  #           'Payday loan': 'Payday loan, title loan, or personal loan',\n",
    "  #           'Money transfer': 'Money transfer, virtual currency, or money service',\n",
    "  #           'Virtual currency': 'Money transfer, virtual currency, or money service'}}, \n",
    "  #          inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf89ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeige der nun vorhandenen Kategorien\n",
    "# pd.DataFrame(df2.Product.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c461db90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW-Vektorisierung\n",
    "# Import der Klasse CountVectorizer für BOW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Erstellen des CountVectorizer-Objekts\n",
    "# Eingrenzung auf 100 Merkmale\n",
    "# Berücksichtigung von Unigramme und Bigramme (ngram_range)\n",
    "# Erstellung einer Liste mit englische und aufgrund der Daten definierten Stoppwörtern\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['xx','xxx','xxxx','00']\n",
    "stopwords.extend(newStopWords)\n",
    "\n",
    "# Entfernung der Stoppwörter aus der Liste für englische Stoppwörter (stop_words)\n",
    "\n",
    "bow = CountVectorizer(max_features=100, min_df=1000, ngram_range=(1, 2), stop_words= stopwords)\n",
    "\n",
    "# Umwandlung der Beschwerden in einen BoW-Vektor\n",
    "bow_features = bow.fit_transform(df2.Complaint).toarray()\n",
    "\n",
    "print(df2.columns)\n",
    "\n",
    "#labels = df2.category_id\n",
    "\n",
    "#Ausagbe der Ergebnisse\n",
    "print(\"Jede der %d Beschwerden wird dargestellt durch %d Merkmale\" %(bow_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c27437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF-Vektorisierung\n",
    "# Import des TFIDF Vektorizers\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Entfernung von Wörtern, welche in weniger als 1000 Dokumenten vorkommen (min_df)\n",
    "# Berücksichtigung von Unigramme und Bigramme (ngram_range)\n",
    "# Entfernung der Stoppwörter aus der Liste für englische Stoppwörter (stop_words) und Datensatzspetifische Stoppwörter\n",
    "# Eingrenzung auf 100 Merkmale\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=100,sublinear_tf=True, min_df=1000,\n",
    "                        ngram_range=(1, 2), \n",
    "                        stop_words=stopwords)\n",
    "\n",
    "# Jede Beschwerde wird in einen TFIDF-Vektor umgewandelt\n",
    "tfidf_features = tfidf.fit_transform(df2.Complaint).toarray()\n",
    "\n",
    "print(df2.columns)\n",
    "\n",
    "#labels = df2.category_id\n",
    "\n",
    "print(\"Jede der %d Beschwerden wird dargestellt durch %d Merkmale\" %(tfidf_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a2956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir sehen nun die Anzahl der Merkmale ist identisch, weil beide Methoden das Vokabular \n",
    "# aus den gleichen Eingabedaten erstellen. Beide Methoden betrachten jedes eindeutige Wort \n",
    "# in den Daten als ein Merkmal.\n",
    "# Die Unterschiede zwischen TF-IDF und BoW liegen in der Art und Weise, wie sie die Werte für diese Merkmale berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b303bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe von 10 Merkmalen für das erste Dokument jeweils für BOW und TF-IDF\n",
    "print(\"BoW Merkmale für das erste Dokument: \", bow_features[20][:30])\n",
    "print(\"TF-IDF Merkmale für das erste Dokument: \", tfidf_features[20][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7cf028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nun erkennt man, dass in großen Textsammlungen viele Nullvektoren bei BOW und TF-IDF entstehen, weshalb nochmals oben\n",
    "# mittels min_df auf noch weniger Merkmale reduziert werden musste, was aber immer noch zu vielen Nullvektoren führt\n",
    "# Auch zu sehen ist in dem Beispiel, dass die Werte in den BoW-Vektoren höher sind da nur die Häufigkeit jedes Wortes \n",
    "# in einem Dokument berücksichtigt wird\n",
    "# während die Werte in den TF-IDF-Vektoren kleiner sind, da die Wortzählungen nach ihrer Bedeutung in den Dokumenten\n",
    "# gewichtet werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7effe9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeige der 100 Merkmale\n",
    "print(\"BoW Merkmale: \", bow.get_feature_names_out())\n",
    "print(\"TF-IDF Merkmale: \", tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb09833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Themenanalyse wird mit den TF-IDF-Vektoren durchgeführt, da diese in der Regel bessere Ergebnisse als BOW liefert\n",
    "# Berechnen der durchschnittlichen TF-IDF-Werte für jedes Wort\n",
    "avg_tfidf = np.mean(tfidf_features, axis=0)\n",
    "\n",
    "# Erstellen eines DataFrames mit den Wörtern und ihren durchschnittlichen TF-IDF-Werten\n",
    "df_tfidf = pd.DataFrame({'word': tfidf.get_feature_names_out(), 'avg_tfidf': avg_tfidf})\n",
    "\n",
    "# Sortieren des DataFrames in absteigender Reihenfolge der durchschnittlichen TF-IDF-Werte\n",
    "df_tfidf = df_tfidf.sort_values('avg_tfidf', ascending=False)\n",
    "\n",
    "# Ausgabe der Wörter und ihrer durchschnittlichen TF-IDF-Werte\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9705d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraktion der 5 häufigsten Themen jeder Kategorie mittels sklearn LDA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Erstellung des LDA-Modells\n",
    "lda_model=LatentDirichletAllocation(n_components=5,learning_method=\n",
    "'online',random_state=42,max_iter=1)\n",
    "\n",
    "# Anpassung des LDA-Models an die Merkmale\n",
    "lda_top=lda_model.fit_transform(tfidf_features)\n",
    "\n",
    "# Ausgabe der Themenverteilung für jede Beschwerde in Prozent\n",
    "for i,topic in enumerate(lda_top[0]):\n",
    "  print(\"Thema \",i,\": \",topic*100,\"%\")\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "N=10\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "  print(\"\\n==> Thema %d:\" %(topic_idx))\n",
    "  print(\"  * Schlüsselwörter: \", \" \".join([feature_names[i] for i in topic.argsort()[:-N - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der beispielhaften Themenverteilung in Prozent der ersten 5 Beschwerden\n",
    "for i, topic_dist in enumerate(lda_top[:5]):\n",
    "  print(\"\\n==> Beschwerde %d:\" %(i))\n",
    "  print(\"  * Themenverteilung: \", topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18716f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraktion der 5 häufigsten Themen jeder Kategorie mittels sklearn LSA\n",
    "\n",
    "# Erstellen des LSA-Modells\n",
    "lsa_model = TruncatedSVD(n_components=5, random_state=42)\n",
    "\n",
    "# Anpassung des LSA-Modells an die Merkmale\n",
    "lsa_top = lsa_model.fit_transform(tfidf_features)\n",
    "\n",
    "# Ausgabe der Themenverteilung für jede Beschwerde\n",
    "for i, topic in enumerate(lsa_top[:5]):\n",
    "    print(\"\\n==> Thema %d:\" %(i))\n",
    "    print(\"  * Themenverteilung: \", topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21fb0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der Schlüsselwörter für jedes Thema für LSA\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lsa_model.components_[:5]):\n",
    "    print(\"\\n==> Thema %d:\" %(topic_idx))\n",
    "    print(\"  * Schlüsselwörter: \", \" \".join([feature_names[i] for i in topic.argsort()[:-N - 1:-1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
